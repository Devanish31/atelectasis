{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe760c9",
   "metadata": {},
   "source": [
    "## Getting CT acquisition data for preoperative and postoperative datasets from the DICOM metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydicom\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# === Input paths ===\n",
    "nii_folder = r\"------ INSERT PATH HERE ------\"\n",
    "dicom_root = r\"------ INSERT PATH HERE ------\"\n",
    "series_excel = r\"------ INSERT PATH HERE ------\"\n",
    "\n",
    "# === Step 1: Get valid MRNs from NIfTI filenames ===\n",
    "cohort_mrns = {f.split('_')[0] for f in os.listdir(nii_folder) if f.endswith('.nii.gz')}\n",
    "\n",
    "# === Step 2: Load Excel to get only the selected series folders ===\n",
    "series_df = pd.read_excel(series_excel, dtype=str)\n",
    "series_df = series_df[series_df['MRN'].isin(cohort_mrns)][['MRN', 'Preop_Series', 'Postop_Series']].dropna()\n",
    "\n",
    "# === Step 3: Scan for metadata from only the chosen series folders ===\n",
    "def extract_metadata_once(series_root):\n",
    "    \"\"\"Extract metadata from the first valid DICOM file found in a folder.\"\"\"\n",
    "    for root, _, files in os.walk(series_root):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                dcm = pydicom.dcmread(file_path, stop_before_pixels=True, force=True)\n",
    "                contrast = (\n",
    "                    \"Yes\" if hasattr(dcm, 'ContrastBolusAgent') and dcm.ContrastBolusAgent not in [\"\", None]\n",
    "                    else \"No\"\n",
    "                )\n",
    "                return {\n",
    "                    \"Manufacturer\": str(getattr(dcm, 'Manufacturer', 'N/A')),\n",
    "                    \"ReconstructionKernel\": str(getattr(dcm, 'ConvolutionKernel', 'N/A')),\n",
    "                    \"KVP\": float(getattr(dcm, 'KVP', np.nan)),\n",
    "                    \"TubeCurrent\": float(getattr(dcm, 'XRayTubeCurrent', np.nan)),\n",
    "                    \"ContrastUsed\": contrast\n",
    "                }\n",
    "            except:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "preop_metadata = []\n",
    "postop_metadata = []\n",
    "\n",
    "for _, row in series_df.iterrows():\n",
    "    mrn = row['MRN']\n",
    "    preop_series = row['Preop_Series']\n",
    "    postop_series = row['Postop_Series']\n",
    "    mrn_path = os.path.join(dicom_root, mrn)\n",
    "\n",
    "    preop_data = None\n",
    "    postop_data = None\n",
    "\n",
    "    # Locate and extract from the exact matching series folder\n",
    "    for root, _, _ in os.walk(mrn_path):\n",
    "        if preop_series in os.path.normpath(root):\n",
    "            preop_data = extract_metadata_once(root)\n",
    "        if postop_series in os.path.normpath(root):\n",
    "            postop_data = extract_metadata_once(root)\n",
    "\n",
    "    if preop_data:\n",
    "        preop_metadata.append(preop_data)\n",
    "    if postop_data:\n",
    "        postop_metadata.append(postop_data)\n",
    "\n",
    "# === Step 4: Summary stats\n",
    "def summarize(metadata):\n",
    "    df = pd.DataFrame(metadata)\n",
    "\n",
    "    def dist(col):\n",
    "        counts = df[col].value_counts(dropna=False)\n",
    "        perc = counts / len(df) * 100\n",
    "        return pd.DataFrame({'Count': counts, 'Percentage': perc.round(2)})\n",
    "\n",
    "    def numeric(col):\n",
    "        values = df[col].dropna()\n",
    "        return {\n",
    "            'Min': float(values.min()),\n",
    "            'Max': float(values.max()),\n",
    "            'Median': float(values.median()),\n",
    "            'IQR': (float(values.quantile(0.25)), float(values.quantile(0.75)))\n",
    "        } if not values.empty else {}\n",
    "\n",
    "    return {\n",
    "        'Manufacturer Distribution': dist('Manufacturer'),\n",
    "        'Kernel Distribution': dist('ReconstructionKernel'),\n",
    "        'KVP Stats': numeric('KVP'),\n",
    "        'TubeCurrent Stats': numeric('TubeCurrent'),\n",
    "        'Contrast Usage': dist('ContrastUsed')\n",
    "    }\n",
    "\n",
    "# === Step 5: Print results\n",
    "preop_summary = summarize(preop_metadata)\n",
    "postop_summary = summarize(postop_metadata)\n",
    "\n",
    "print(\"\\n=== PREOPERATIVE CT ACQUISITION SUMMARY ===\")\n",
    "print(\"Manufacturers:\\n\", preop_summary['Manufacturer Distribution'])\n",
    "print(\"\\nReconstruction Kernels:\\n\", preop_summary['Kernel Distribution'])\n",
    "print(\"\\nKVP:\", preop_summary['KVP Stats'])\n",
    "print(\"Tube Current:\", preop_summary['TubeCurrent Stats'])\n",
    "print(\"\\nContrast Usage:\\n\", preop_summary['Contrast Usage'])  # <-- ADD THIS LINE\n",
    "\n",
    "print(\"\\n=== POSTOPERATIVE CT ACQUISITION SUMMARY ===\")\n",
    "print(\"Manufacturers:\\n\", postop_summary['Manufacturer Distribution'])\n",
    "print(\"\\nReconstruction Kernels:\\n\", postop_summary['Kernel Distribution'])\n",
    "print(\"\\nKVP:\", postop_summary['KVP Stats'])\n",
    "print(\"Tube Current:\", postop_summary['TubeCurrent Stats'])\n",
    "print(\"\\nContrast Usage:\\n\", postop_summary['Contrast Usage'])  # <-- AND THIS LINE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0e170",
   "metadata": {},
   "source": [
    "## Counting manufacturer metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807739af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paths ===\n",
    "excel_path = r\"------ INSERT PATH HERE ------\"\n",
    "dicom_base = r\"------ INSERT PATH HERE ------\"\n",
    "\n",
    "# === Load Excel File ===\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# === Helper to find scanner type from series folder ===\n",
    "def find_scanner_type(series_name, mrn):\n",
    "    base_path = os.path.join(dicom_base, str(mrn), \"DICOM\")\n",
    "    if not os.path.exists(base_path):\n",
    "        return None\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if os.path.basename(root) == str(series_name):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    dcm = pydicom.dcmread(file_path, stop_before_pixels=True, force=True)\n",
    "                    return dcm.get(\"Manufacturer\", \"Unknown\")\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return None\n",
    "\n",
    "# === Process all rows ===\n",
    "preop_scanners = []\n",
    "postop_scanners = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    mrn = row[\"MRN\"]\n",
    "    preop_series = row[\"Preop_Series\"]\n",
    "    postop_series = row[\"Postop_Series\"]\n",
    "\n",
    "    preop_scanners.append(find_scanner_type(preop_series, mrn))\n",
    "    postop_scanners.append(find_scanner_type(postop_series, mrn))\n",
    "\n",
    "# === Add to DataFrame ===\n",
    "df[\"Preop_Scanner\"] = preop_scanners\n",
    "df[\"Postop_Scanner\"] = postop_scanners\n",
    "\n",
    "# === Compute counts and percentages ===\n",
    "preop_counts = df[\"Preop_Scanner\"].value_counts(normalize=True) * 100\n",
    "postop_counts = df[\"Postop_Scanner\"].value_counts(normalize=True) * 100\n",
    "\n",
    "# === Print percentages ===\n",
    "print(\"\\nðŸ“Š Preop Scanner Manufacturer Distribution:\")\n",
    "print(preop_counts.round(2).to_string())\n",
    "\n",
    "print(\"\\nðŸ“Š Postop Scanner Manufacturer Distribution:\")\n",
    "print(postop_counts.round(2).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273264bd",
   "metadata": {},
   "source": [
    "## Sensitivity analysis - Dice scores for 2.5mm or more slice thickness scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c235ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paths ===\n",
    "preop_pred_dir = r\"------ INSERT PATH HERE ------\"\n",
    "postop_pred_dir = r\"------ INSERT PATH HERE ------\"\n",
    "preop_gt_dir = r\"------ INSERT PATH HERE ------\"\n",
    "postop_gt_dir = r\"------ INSERT PATH HERE ------\"\n",
    "slicefile = r\"------ INSERT PATH HERE ------\"\n",
    "\n",
    "# === Load slice thickness data ===\n",
    "df_slices = pd.read_excel(slicefile)\n",
    "df_slices['MRN'] = df_slices['MRN'].astype(str)\n",
    "\n",
    "# === Dice calculation ===\n",
    "def dice(pred, gt):\n",
    "    intersection = np.sum(pred & gt)\n",
    "    total = np.sum(pred) + np.sum(gt)\n",
    "    if total == 0:\n",
    "        return 1.0\n",
    "    return 2.0 * intersection / total\n",
    "\n",
    "def get_mrn_from_filename(fname):\n",
    "    return ''.join(filter(str.isdigit, fname.split('.')[0]))\n",
    "\n",
    "def compute_dice_scores(pred_dir, gt_dir, slice_col, labels, label_map):\n",
    "    scores = {l: [] for l in labels}\n",
    "    scores['mean'] = []\n",
    "    per_scan = []  # To store per-scan Dice values\n",
    "\n",
    "    for fname in os.listdir(pred_dir):\n",
    "        if not fname.endswith(\".nii.gz\"):\n",
    "            continue\n",
    "        mrn = get_mrn_from_filename(fname)\n",
    "        row = df_slices[df_slices['MRN'] == mrn]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        thickness = row[slice_col].values[0]\n",
    "        if pd.isna(thickness) or thickness < 2.5:\n",
    "            continue\n",
    "\n",
    "        pred_path = os.path.join(pred_dir, fname)\n",
    "        gt_path = os.path.join(gt_dir, fname.replace(\"_postprocessed\", \"\").replace(\"_final\", \"\"))\n",
    "        if not os.path.exists(gt_path):\n",
    "            continue\n",
    "\n",
    "        pred = nib.load(pred_path).get_fdata().astype(np.uint8)\n",
    "        gt = nib.load(gt_path).get_fdata().astype(np.uint8)\n",
    "\n",
    "        per_label_dice = {}\n",
    "        for label_id in labels:\n",
    "            label_name = label_map[label_id]\n",
    "            d = dice(pred == label_id, gt == label_id)\n",
    "            scores[label_id].append(d)\n",
    "            per_label_dice[label_name] = d\n",
    "        mean_d = np.mean(list(per_label_dice.values()))\n",
    "        scores['mean'].append(mean_d)\n",
    "        per_scan.append((mrn, per_label_dice, mean_d))\n",
    "    return scores, per_scan\n",
    "\n",
    "def summarize(scores, per_scan, label_names, title):\n",
    "    print(f\"\\nðŸ“Š {title} (Slice Thickness â‰¥ 2.5 mm):\\n\")\n",
    "\n",
    "    for mrn, scan_scores, mean_d in per_scan:\n",
    "        label_str = ', '.join([f\"{k}: {v:.3f}\" for k, v in scan_scores.items()])\n",
    "        print(f\"ðŸ§ª MRN {mrn} â†’ {label_str} | Mean: {mean_d:.3f}\")\n",
    "\n",
    "    print(\"\\nðŸ“ˆ Summary:\")\n",
    "    for label in label_names:\n",
    "        arr = np.array(scores[label])\n",
    "        mean = np.nanmean(arr)\n",
    "        std = np.nanstd(arr)\n",
    "        print(f\"  {label}: {mean:.3f} Â± {std:.3f}\")\n",
    "    mean_overall = np.nanmean(scores['mean'])\n",
    "    std_overall = np.nanstd(scores['mean'])\n",
    "    print(f\"  Overall Mean: {mean_overall:.3f} Â± {std_overall:.3f}\")\n",
    "\n",
    "# === Run\n",
    "preop_labels = ['RUL', 'RML', 'RLL', 'LUL', 'LLL']\n",
    "preop_map = {1: 'RUL', 2: 'RML', 3: 'RLL', 4: 'LUL', 5: 'LLL'}\n",
    "postop_labels = ['RML', 'RLL', 'LLL']\n",
    "postop_map = {1: 'RML', 2: 'RLL', 3: 'LLL'}\n",
    "\n",
    "label_ids_preop = list(preop_map.keys())\n",
    "label_ids_postop = list(postop_map.keys())\n",
    "\n",
    "# Compute\n",
    "preop_scores, preop_per_scan = compute_dice_scores(preop_pred_dir, preop_gt_dir, 'Preop_SliceThickness', label_ids_preop, preop_map)\n",
    "postop_scores, postop_per_scan = compute_dice_scores(postop_pred_dir, postop_gt_dir, 'Postop_SliceThickness', label_ids_postop, postop_map)\n",
    "\n",
    "# Rename keys for printing\n",
    "preop_scores_named = {preop_map[k]: v for k, v in preop_scores.items() if k != 'mean'}\n",
    "preop_scores_named['mean'] = preop_scores['mean']\n",
    "\n",
    "postop_scores_named = {postop_map[k]: v for k, v in postop_scores.items() if k != 'mean'}\n",
    "postop_scores_named['mean'] = postop_scores['mean']\n",
    "\n",
    "# Print\n",
    "summarize(preop_scores_named, preop_per_scan, preop_labels, \"Preop Dice Scores\")\n",
    "summarize(postop_scores_named, postop_per_scan, postop_labels, \"Postop Dice Scores\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2d641",
   "metadata": {},
   "source": [
    "## Atelectasis grading and plotting using different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "file_path = r\"------ INSERT PATH HERE ------\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# === Variables ===\n",
    "grade_cols = [\n",
    "    \"CT_Atelectasis_6m\",  # top row\n",
    "    \"CT_Atelectasis_6m_0_12_34\"   # bottom row\n",
    "]\n",
    "metrics = [\"Delta_RML\", \"Delta_RML_RL\", \"Delta_RML_TL\", \"Delta_RLL_RL\", \"Delta_RLL_TL\"]\n",
    "titles = [\"Î”RML\", \"Î”RML/RL\", \"Î”RML/TL\", \"Î”RLL/RL\", \"Î”RLL/TL\"]\n",
    "\n",
    "# === Plot setup ===\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(2, 5, figsize=(24, 10), sharey=False)\n",
    "\n",
    "for row_idx, grade_col in enumerate(grade_cols):\n",
    "    for col_idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        sns.boxplot(x=grade_col, y=metric, data=df, ax=ax, showfliers=False)\n",
    "        sns.stripplot(x=grade_col, y=metric, data=df, ax=ax, color='black', size=4, jitter=True, alpha=0.7)\n",
    "        \n",
    "        if row_idx == 0:\n",
    "            ax.set_title(title, fontsize=13)\n",
    "        ax.set_xlabel(\"Atelectasis Grade\", fontsize=11)\n",
    "        ax.set_ylabel(metric if col_idx == 0 else \"\", fontsize=11)\n",
    "\n",
    "        # Annotate p-values between adjacent grades\n",
    "        unique_grades = sorted(df[grade_col].dropna().unique())\n",
    "        num_tests = len(unique_grades) - 1\n",
    "        alpha_bonf = 0.05 / num_tests if num_tests > 0 else 1\n",
    "\n",
    "        for j in range(num_tests):\n",
    "            g1, g2 = unique_grades[j], unique_grades[j + 1]\n",
    "            vals1 = df[df[grade_col] == g1][metric].dropna()\n",
    "            vals2 = df[df[grade_col] == g2][metric].dropna()\n",
    "            if len(vals1) > 1 and len(vals2) > 1:\n",
    "                stat, p = mannwhitneyu(vals1, vals2, alternative='two-sided')\n",
    "                y_max = df[metric].dropna().max()\n",
    "                y = y_max + 0.05 * (j + 1) * y_max\n",
    "                ax.plot([j, j + 1], [y, y], color='black')\n",
    "                if p < 0.001:\n",
    "                    p_text = \"p < 0.001\"\n",
    "                else:\n",
    "                    p_text = f\"p={p:.3f}\"\n",
    "                if p < alpha_bonf:\n",
    "                    p_text += \" *\"\n",
    "                ax.text((j + j + 1) / 2, y, p_text, ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a849baa2",
   "metadata": {},
   "source": [
    "## Sensitivity analysis - Removing +RML wedge resection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6def0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load data ===\n",
    "file_path = r\"------ INSERT PATH HERE ------\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# === Exclude rows where RML_w == \"w\" ===\n",
    "df = df[df[\"RML_w\"] != \"w\"]\n",
    "\n",
    "# === Variables ===\n",
    "grade_cols = [\n",
    "    \"CT_Atelectasis_6m\",  # top row\n",
    "    \"CT_Atelectasis_6m_0_12_34\"   # bottom row\n",
    "]\n",
    "metrics = [\"Delta_RML\", \"Delta_RML_RL\", \"Delta_RML_TL\", \"Delta_RLL_RL\", \"Delta_RLL_TL\"]\n",
    "titles = [\"Î”RML\", \"Î”RML RL\", \"Î”RML TL\", \"Î”RLL RL\", \"Î”RLL TL\"]\n",
    "\n",
    "# === Plot setup ===\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(2, 5, figsize=(24, 10), sharey=False)\n",
    "\n",
    "for row_idx, grade_col in enumerate(grade_cols):\n",
    "    for col_idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        sns.boxplot(x=grade_col, y=metric, data=df, ax=ax, showfliers=False)\n",
    "        sns.stripplot(x=grade_col, y=metric, data=df, ax=ax, color='black', size=4, jitter=True, alpha=0.7)\n",
    "        \n",
    "        if row_idx == 0:\n",
    "            ax.set_title(title, fontsize=13)\n",
    "        ax.set_xlabel(\"Atelectasis Grade\", fontsize=11)\n",
    "        ax.set_ylabel(metric if col_idx == 0 else \"\", fontsize=11)\n",
    "\n",
    "        # Annotate p-values between adjacent grades\n",
    "        unique_grades = sorted(df[grade_col].dropna().unique())\n",
    "        num_tests = len(unique_grades) - 1\n",
    "        alpha_bonf = 0.05 / num_tests if num_tests > 0 else 1\n",
    "\n",
    "        for j in range(num_tests):\n",
    "            g1, g2 = unique_grades[j], unique_grades[j + 1]\n",
    "            vals1 = df[df[grade_col] == g1][metric].dropna()\n",
    "            vals2 = df[df[grade_col] == g2][metric].dropna()\n",
    "            if len(vals1) > 1 and len(vals2) > 1:\n",
    "                stat, p = mannwhitneyu(vals1, vals2, alternative='two-sided')\n",
    "                y_max = df[metric].dropna().max()\n",
    "                y = y_max + 0.05 * (j + 1) * y_max\n",
    "                ax.plot([j, j + 1], [y, y], color='black')\n",
    "                if p < 0.001:\n",
    "                    p_text = \"p < 0.001\"\n",
    "                else:\n",
    "                    p_text = f\"p={p:.3f}\"\n",
    "                if p < alpha_bonf:\n",
    "                    p_text += \" *\"\n",
    "                ax.text((j + j + 1) / 2, y, p_text, ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c223435",
   "metadata": {},
   "source": [
    "## Getting distribution plots for the HU values for two training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paths ===\n",
    "img_dir = r\"------ INSERT PATH HERE ------\"\n",
    "mask_dir = r\"------ INSERT PATH HERE ------\"\n",
    "\n",
    "# === Initialization ===\n",
    "label_range = [1, 2, 3, 4, 5]\n",
    "data = []\n",
    "\n",
    "# === Loop over CTs ===\n",
    "for fname in os.listdir(img_dir):\n",
    "    if not fname.endswith(\"_0000.nii.gz\"):\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(img_dir, fname)\n",
    "    base_id = fname.replace(\"_0000.nii.gz\", \"\")\n",
    "    dataset_type = \"LLS\" if base_id.startswith(\"LLS\") else \"MRN\"\n",
    "\n",
    "    mask_fname = base_id + \".nii.gz\"\n",
    "    mask_path = os.path.join(mask_dir, mask_fname)\n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"Missing mask for: {base_id}\")\n",
    "        continue\n",
    "\n",
    "    img = nib.load(img_path).get_fdata()\n",
    "    mask = nib.load(mask_path).get_fdata()\n",
    "\n",
    "    for label in label_range:\n",
    "        hu_vals = img[mask == label].flatten()\n",
    "        if len(hu_vals) == 0:\n",
    "            continue\n",
    "        sampled_vals = np.random.choice(hu_vals, size=min(5000, len(hu_vals)), replace=False)\n",
    "        data.extend([(v, dataset_type) for v in sampled_vals])\n",
    "\n",
    "# === Create DataFrame ===\n",
    "df = pd.DataFrame(data, columns=[\"HU\", \"Dataset\"])\n",
    "df[\"Dataset\"] = df[\"Dataset\"].replace({\n",
    "    \"LLS\": \"Ottawa chest CTs\",\n",
    "    \"MRN\": \"Stanford chest CTs\"\n",
    "})\n",
    "\n",
    "# === Manually ensure both categories exist in the plot (even with dummy data if needed) ===\n",
    "required_datasets = [\"Ottawa chest CTs\", \"Stanford chest CTs\"]\n",
    "for req in required_datasets:\n",
    "    if req not in df[\"Dataset\"].values:\n",
    "        df = pd.concat([df, pd.DataFrame({\"HU\": [-2000], \"Dataset\": [req]})], ignore_index=True)\n",
    "\n",
    "# === Plot ===\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=df, x=\"HU\", hue=\"Dataset\", fill=True, common_norm=False, alpha=0.5)\n",
    "\n",
    "plt.title(\"Distribution of HU Values Across Dataset Types (All Lobes Combined)\")\n",
    "plt.xlabel(\"HU Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend(title=\"Dataset Type\")\n",
    "plt.xlim(-1100, 500)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
